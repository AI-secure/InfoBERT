# InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective 

This is the official code base for our ICLR 2021 paper:

["InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective".](https://openreview.net/forum?id=hpH98mK5Puk)

Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, Jingjing Liu

## Usage
### Prepare your environment 

Download required packages
```
pip install -r requirements.txt
```
### ANLI and TextFooler
To run ANLI and TextFooler experiments, refer to [README](https://github.com/AI-secure/InfoBERT/tree/master/ANLI) in the `ANLI` directory.

### SQuAD
To run SQuAD experiments, refer to [README](https://github.com/AI-secure/InfoBERT/tree/master/SQuAD) in the `SQuAD` directory.

## Citation
```
@inproceedings{
wang2021infobert,
title={InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective},
author={Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
booktitle={International Conference on Learning Representations},
year={2021}}
```